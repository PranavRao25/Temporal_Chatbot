{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9BS0syAa6FY"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM, BertTokenizer, RobertaForMaskedLM, RobertaTokenizer, DistilBertForMaskedLM, DistilBertTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "JDqcsRJZbqTC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_base = \"google-bert/bert-base-uncased\"\n",
        "roberta_base = \"FacebookAI/roberta-base\"\n",
        "distilbert_base = \"distilbert/distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "Hq2j_oVjbVQG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "roberta_model = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "distilbert_model = DistilBertForMaskedLM.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "URULm6IYgKeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_model_language(model, tokenizer):\n",
        "  def masked_prediction(sentence, top_k=5):\n",
        "    # Tokenize the input text and identify the masked token's position\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "    # Get model outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # The prediction logits for the masked tokens\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Get the predictions for the masked position\n",
        "    mask_token_logits = logits[0, mask_token_index, :]\n",
        "    top_5_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n",
        "\n",
        "    # Convert token ids to words\n",
        "    predicted_words = [tokenizer.decode([token]) for token in top_5_tokens]\n",
        "\n",
        "    return predicted_words\n",
        "  return masked_prediction"
      ],
      "metadata": {
        "id": "McdPa0lRexq5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_base_prediction = mask_model_language(bert_model, bert_tokenizer)\n",
        "roberta_base_prediction = mask_model_language(roberta_model, roberta_tokenizer)\n",
        "distilbert_base_prediction = mask_model_language(distilbert_model, distilbert_tokenizer)"
      ],
      "metadata": {
        "id": "GiVJSroFfeBt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhJz_CBbY4Yj",
        "outputId": "f983c2be-7374-40bd-8be4-08a0a61450d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "B5WUOU9lZaoo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_path1 = \"/content/drive/MyDrive/Temporal Chatbot BTP/BTP Datasets/temporal_ordering_changed2.json\"\n",
        "# dataset_path = \"/content/drive/MyDrive/Temporal Chatbot BTP/BTP Datasets/temp_temporal_graph_data.json\"\n",
        "# dataset_path2 = \"/content/drive/MyDrive/Temporal Chatbot BTP/BTP Datasets/temporal_ordering_split_changed.json\""
      ],
      "metadata": {
        "id": "G2OVKS4aZslz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(dataset_path, \"r\") as f:\n",
        "#   dataset = json.load(f)"
      ],
      "metadata": {
        "id": "tbpziBvWZ1Aw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_dataset = dict()\n",
        "# for i in dataset:\n",
        "#     new_dataset[i] = dict()\n",
        "#     for j in dataset[i]:\n",
        "#         new_dataset[i][j] = dataset[i][j] if j == 'ngbs' else {\"time_value\": dataset[i][j][0], \"time_duration\": dataset[i][j][1]}"
      ],
      "metadata": {
        "id": "rk3HyNEgf9aX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/drive/MyDrive/Temporal Chatbot BTP/BTP Datasets/temp_temporal_graph_data1.json\", \"w\") as f:\n",
        "#   json.dump(new_dataset, f)"
      ],
      "metadata": {
        "id": "xzGOZBNhhoAJ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lines = []\n",
        "# for current_event in new_dataset:\n",
        "#     current_line = f\"{current_event} -> \"\n",
        "#     next_events = new_dataset[current_event]['ngbs']\n",
        "#     for next_event in next_events:\n",
        "#         current_line += f\"{next_event}, \"\n",
        "#     for time_values in new_dataset[current_event]['time_point']:\n",
        "#         if time_values == 'time_value':\n",
        "#             current_line += f\": Time value = {new_dataset[current_event]['time_point'][time_values][1]} \"\n",
        "#         else:\n",
        "#             current_line += f\": Time duration = {new_dataset[current_event]['time_point'][time_values][1]} \"\n",
        "#     lines.append(current_line)"
      ],
      "metadata": {
        "id": "pyaNRU1Mi13B"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_masked_time_value_lines = []\n",
        "# for current_event in new_dataset:\n",
        "#     current_data = dict()\n",
        "\n",
        "#     current_line = f\"{current_event} -> \"\n",
        "#     for next_event in new_dataset[current_event]['ngbs']:\n",
        "#         current_line += f\"{next_event}, \"\n",
        "\n",
        "#     time_value = new_dataset[current_event]['time_point']['time_value'][1]\n",
        "#     time_duration = new_dataset[current_event]['time_point']['time_duration'][1]\n",
        "#     current_line += f\": Time value =  [MASK] \"\n",
        "#     current_line += f\": Time duration = {time_duration} \"\n",
        "\n",
        "#     current_data[current_event] = current_line\n",
        "#     current_data['mask_value'] = time_value\n",
        "#     bert_masked_time_value_lines.append(current_data)\n",
        "\n",
        "# bert_masked_time_duration_lines = []\n",
        "# for current_event in new_dataset:\n",
        "#     current_data = dict()\n",
        "\n",
        "#     current_line = f\"{current_event} -> \"\n",
        "#     for next_event in new_dataset[current_event]['ngbs']:\n",
        "#         current_line += f\"{next_event}, \"\n",
        "\n",
        "#     time_value = new_dataset[current_event]['time_point']['time_value'][1]\n",
        "#     time_duration = new_dataset[current_event]['time_point']['time_duration'][1]\n",
        "#     current_line += f\": Time value = {time_value} \"\n",
        "#     current_line += f\": Time duration =  [MASK] \"\n",
        "\n",
        "#     current_data[current_event] = current_line\n",
        "#     current_data['mask_value'] = time_value\n",
        "#     bert_masked_time_duration_lines.append(current_data)\n",
        "\n",
        "# bert_masked_next_event = []\n",
        "# for current_event in new_dataset:\n",
        "#     time_value = new_dataset[current_event]['time_point']['time_value'][1]\n",
        "#     time_duration = new_dataset[current_event]['time_point']['time_duration'][1]\n",
        "\n",
        "#     for i in range(len(new_dataset[current_event]['ngbs'])):\n",
        "#         current_line = \\\n",
        "#         f\"{current_event} -> \" +\\\n",
        "#         \", \".join([next_event for next_event in new_dataset[current_event]['ngbs'][:i]]) +\\\n",
        "#         \", [MASK], \" +\\\n",
        "#         \", \".join([next_event for next_event in new_dataset[current_event]['ngbs'][i+1:]]) +\\\n",
        "#         f\": Time value = {time_value} \" +\\\n",
        "#         f\": Time duration = {time_duration} \"\n",
        "#         event = new_dataset[current_event]['ngbs'][i]\n",
        "#         bert_masked_next_event.append({\n",
        "#             current_event: current_line,\n",
        "#             'mask_value': event\n",
        "#         })\n",
        "\n",
        "# roberta_masked_time_value_lines = []\n",
        "# for current_event in new_dataset:\n",
        "#     current_data = dict()\n",
        "\n",
        "#     current_line = f\"{current_event} -> \"\n",
        "#     for next_event in new_dataset[current_event]['ngbs']:\n",
        "#         current_line += f\"{next_event}, \"\n",
        "\n",
        "#     time_value = new_dataset[current_event]['time_point']['time_value'][1]\n",
        "#     time_duration = new_dataset[current_event]['time_point']['time_duration'][1]\n",
        "#     current_line += f\": Time value =  <MASK> \"\n",
        "#     current_line += f\": Time duration = {time_duration} \"\n",
        "\n",
        "#     current_data[current_event] = current_line\n",
        "#     current_data['mask_value'] = time_value\n",
        "#     roberta_masked_time_value_lines.append(current_data)\n",
        "\n",
        "# roberta_masked_time_duration_lines = []\n",
        "# for current_event in new_dataset:\n",
        "#     current_data = dict()\n",
        "\n",
        "#     current_line = f\"{current_event} -> \"\n",
        "#     for next_event in new_dataset[current_event]['ngbs']:\n",
        "#         current_line += f\"{next_event}, \"\n",
        "\n",
        "#     time_value = new_dataset[current_event]['time_point']['time_value'][1]\n",
        "#     time_duration = new_dataset[current_event]['time_point']['time_duration'][1]\n",
        "#     current_line += f\": Time value = {time_value} \"\n",
        "#     current_line += f\": Time duration =  <MASK> \"\n",
        "\n",
        "#     current_data[current_event] = current_line\n",
        "#     current_data['mask_value'] = time_value\n",
        "#     roberta_masked_time_duration_lines.append(current_data)\n",
        "\n",
        "# roberta_masked_next_event = []\n",
        "# for current_event in new_dataset:\n",
        "#     time_value = new_dataset[current_event]['time_point']['time_value'][1]\n",
        "#     time_duration = new_dataset[current_event]['time_point']['time_duration'][1]\n",
        "\n",
        "#     for i in range(len(new_dataset[current_event]['ngbs'])):\n",
        "#         current_line = \\\n",
        "#         f\"{current_event} -> \" +\\\n",
        "#         \", \".join([next_event for next_event in new_dataset[current_event]['ngbs'][:i]]) +\\\n",
        "#         \", <MASK>, \" +\\\n",
        "#         \", \".join([next_event for next_event in new_dataset[current_event]['ngbs'][i+1:]]) +\\\n",
        "#         f\": Time value = {time_value} \" +\\\n",
        "#         f\": Time duration = {time_duration} \"\n",
        "#         event = new_dataset[current_event]['ngbs'][i]\n",
        "#         bert_masked_next_event.append({\n",
        "#             current_event: current_line,\n",
        "#             'mask_value': event\n",
        "#         })"
      ],
      "metadata": {
        "id": "K9viAHwmkPxH"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# masked_dataset = {\n",
        "#     \"bert_masked_time_value\": bert_masked_time_value_lines,\n",
        "#     \"bert_masked_time_duration\": bert_masked_time_duration_lines,\n",
        "#     \"roberta_masked_time_value\": roberta_masked_time_value_lines,\n",
        "#     \"roberta_masked_time_duration\": roberta_masked_time_duration_lines,\n",
        "#     \"bert_masked_next_event\": bert_masked_next_event,\n",
        "#     \"roberta_masked_next_event\": roberta_masked_next_event\n",
        "# }"
      ],
      "metadata": {
        "id": "JVGV7Anxl1_H"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/drive/MyDrive/Temporal Chatbot BTP/BTP Datasets/masked_dataset.json\", \"w\") as f:\n",
        "#   json.dump(masked_dataset, f)"
      ],
      "metadata": {
        "id": "PnS-U5Ksn-zt"
      },
      "execution_count": 95,
      "outputs": []
    }
  ]
}