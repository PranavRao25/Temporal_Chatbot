{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0F7TJd_84ES"
      },
      "source": [
        "# Required Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-03-16T09:38:24.663449Z",
          "iopub.status.busy": "2024-03-16T09:38:24.662113Z",
          "iopub.status.idle": "2024-03-16T09:38:25.304270Z",
          "shell.execute_reply": "2024-03-16T09:38:25.303221Z",
          "shell.execute_reply.started": "2024-03-16T09:38:24.663408Z"
        },
        "id": "Btbj01xh84EW"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import spacy\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-16T09:38:33.693914Z",
          "iopub.status.busy": "2024-03-16T09:38:33.693215Z",
          "iopub.status.idle": "2024-03-16T09:38:34.116020Z",
          "shell.execute_reply": "2024-03-16T09:38:34.114776Z",
          "shell.execute_reply.started": "2024-03-16T09:38:33.693881Z"
        },
        "id": "TYWXcW8L84EY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('dataset.csv') #,index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-16T09:38:34.119011Z",
          "iopub.status.busy": "2024-03-16T09:38:34.118472Z",
          "iopub.status.idle": "2024-03-16T09:40:44.071125Z",
          "shell.execute_reply": "2024-03-16T09:40:44.069786Z",
          "shell.execute_reply.started": "2024-03-16T09:38:34.118968Z"
        },
        "id": "lukjECS484EZ"
      },
      "outputs": [],
      "source": [
        "! pip install datasets\n",
        "! pip install transformers[torch]\n",
        "! pip install tokenizers\n",
        "! pip install evaluate\n",
        "! pip install rouge_score\n",
        "! pip install sentencepiece\n",
        "! pip install huggingface_hub\n",
        "!pip install timexy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-03-16T09:40:44.075338Z",
          "iopub.status.busy": "2024-03-16T09:40:44.074789Z",
          "iopub.status.idle": "2024-03-16T09:41:05.950783Z",
          "shell.execute_reply": "2024-03-16T09:41:05.948831Z",
          "shell.execute_reply.started": "2024-03-16T09:40:44.075288Z"
        },
        "id": "9jRIzIbe84Ea"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, DataCollatorForSeq2Seq,TFMT5ForConditionalGeneration, MT5Tokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "import timexy\n",
        "from timexy import Timexy\n",
        "from datasets import Dataset, DatasetDict\n",
        "from timexy import rule\n",
        "from timexy.languages import en\n",
        "import tensorflow\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-03-16T09:41:05.954592Z",
          "iopub.status.busy": "2024-03-16T09:41:05.953052Z",
          "iopub.status.idle": "2024-03-16T09:41:07.514554Z",
          "shell.execute_reply": "2024-03-16T09:41:07.513225Z",
          "shell.execute_reply.started": "2024-03-16T09:41:05.954543Z"
        },
        "id": "pedjXZEO84Eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebe7b62-1482-42a6-e83d-83c7d6918b93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<timexy.timexy.Timexy at 0x7849728c10c0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Optionally add config if varying from default values\n",
        "config = {\n",
        "    \"kb_id_type\": \"timex3\",  # possible values: 'timex3'(default), 'timestamp'\n",
        "    \"label\": \"timexy\",       # default: 'timexy'\n",
        "    \"overwrite\": False       # default: False\n",
        "}\n",
        "nlp.add_pipe(\"timexy\", config=config, before=\"ner\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\", quiet=True)"
      ],
      "metadata": {
        "id": "9EWf3jMoFJCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd15f6bd-2965-4ce0-ba9c-eebdda651786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDrY8JHr84Ed"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xds7iifO84Ef"
      },
      "outputs": [],
      "source": [
        "def preprocess(batch):\n",
        "    clean_batch = []\n",
        "    for i in batch['Inputs']:\n",
        "        for j in i.split('\\n')[:-1]:\n",
        "            x = re.split('~__(True|False)__',j)\n",
        "            sent = x[0]\n",
        "            if(eval(x[1])): # temporal\n",
        "              sent = \"(The following sentence has some temporal information.) \" + j\n",
        "            else:\n",
        "              sent = \"(The following sentence has no temporal information.) \" + j\n",
        "            clean_batch.append(sent)\n",
        "    inputs = [\"Generate : \" + sentence for sentence in clean_batch]\n",
        "\n",
        "    model_inputs = tokenizer(inputs,truncation=False,padding=True)\n",
        "    labels = tokenizer(text_target=list(batch['Outputs']),truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds,refs = eval_preds\n",
        "\n",
        "    decode_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
        "\n",
        "    pred_tags = nlp(decode_preds)\n",
        "    actual_tags = [map(lambda x: tokenized_dataset.index(x),decoded_labels)]\n",
        "\n",
        "    err = nltk.metrics.scores.log_likelihood(actual_tags,pred_tags)\n",
        "\n",
        "    return err"
      ],
      "metadata": {
        "id": "IJoJ7tInDSCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvUMq-0e84Eh"
      },
      "outputs": [],
      "source": [
        "# train (1081)\n",
        "c=0\n",
        "for index, row in df.iterrows():\n",
        "    if((row['Chunk'])<=1081):\n",
        "        c+=1\n",
        "train = df.iloc[:c,:]\n",
        "\n",
        "# validate (rest)\n",
        "test = df.iloc[c:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8SP2aeX84Ek"
      },
      "source": [
        "# Training With T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONxCM4-J84Em"
      },
      "outputs": [],
      "source": [
        "# Run it the first time\n",
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "chkpnt = './input/checkpoints/chkpnt1'\n",
        "model.save_pretrained(chkpnt)\n",
        "tokenizer.save_pretrained(chkpnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I_uCgeL84Eo"
      },
      "outputs": [],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(chkpnt)\n",
        "tokenizer = T5Tokenizer.from_pretrained(chkpnt)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = preprocess(train)"
      ],
      "metadata": {
        "id": "WrGcd976qx8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = preprocess(test)"
      ],
      "metadata": {
        "id": "W2vAeqn0RMmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpUQFiFy84Eq"
      },
      "outputs": [],
      "source": [
        "# Training Arguments\n",
        "L_RATE = 3e-4\n",
        "BATCH_SIZE = 4\n",
        "PER_DEVICE_EVAL_BATCH = 2\n",
        "WEIGHT_DECAY = 0.01\n",
        "SAVE_TOTAL_LIM = 3\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "   output_dir=\"./results\",\n",
        "   evaluation_strategy=\"epoch\",\n",
        "   learning_rate=L_RATE,\n",
        "   per_device_train_batch_size=BATCH_SIZE,\n",
        "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
        "   weight_decay=WEIGHT_DECAY,\n",
        "   save_total_limit=SAVE_TOTAL_LIM,\n",
        "   num_train_epochs=NUM_EPOCHS,\n",
        "   predict_with_generate=True,\n",
        "   push_to_hub=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FlwM-1S84Er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078168bc-4335-4a1a-ad20-4c314ac1ca9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_dataset,\n",
        "   eval_dataset=test_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "amGuFZiPNubt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenized_dataset[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL8n1DwsqRg8",
        "outputId": "af9330fe-7539-4dac-e693-18022b53c6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140284"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lO2tWIpbtV0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training With T5 2"
      ],
      "metadata": {
        "id": "VrxzTd44wkXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = TFMT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
      ],
      "metadata": {
        "id": "2tTqLs1ewmNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "            tokenizer,\n",
        "            model=model,\n",
        "            label_pad_token_id=tokenizer.pad_token_id,\n",
        "            pad_to_multiple_of=64,\n",
        "            return_tensors=\"np\",\n",
        "        )"
      ],
      "metadata": {
        "id": "_JWYEfArxD3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(df).map(preprocess, batched=True, desc=\"Running tokenizer\")"
      ],
      "metadata": {
        "id": "JpSt2lkZxlzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "            train_dataset,\n",
        "            collate_fn=data_collator,\n",
        "            batch_size=8,\n",
        "            shuffle=True,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "KttzEGG0xHr4",
        "outputId": "c499f3dc-eab4-4e94-a51f-f5d95a03bca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Dataset argument should be a datasets.Dataset!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-539fda878f2f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tf_train_dataset = model.prepare_tf_dataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mprepare_tf_dataset\u001b[0;34m(self, dataset, batch_size, shuffle, tokenizer, collate_fn, collate_fn_args, drop_remainder, prefetch)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset argument should be a datasets.Dataset!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mmodel_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dataset argument should be a datasets.Dataset!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"Inputs\"].iloc[1])\n",
        "for i in train[\"Inputs\"].iloc[1].split('\\n'):\n",
        "  x = re.split('~__(True|False)__',i)\n",
        "  print(type(x[0]))"
      ],
      "metadata": {
        "id": "m8OKQMuxxdHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with Auto T5"
      ],
      "metadata": {
        "id": "KEPnRMKs0DCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "kDz9X3Mk0Fm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = preprocess(train)"
      ],
      "metadata": {
        "id": "1xf9NDcT09DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "YsH_DPHW1T05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{MODEL_NAME}-finetuned-xsum\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size = batch_size,\n",
        "    weight_decay = 0.01,\n",
        "    save_total_limit = 3,\n",
        "    num_train_epochs = 1,\n",
        "    predict_with_generate = True,\n",
        "    fp16 = True,\n",
        "    push_to_hub = False\n",
        ")"
      ],
      "metadata": {
        "id": "i7ADmMzJ1hIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer,model=model)"
      ],
      "metadata": {
        "id": "UsKFB28f2nVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset = tokenized_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "bKdTxWkp339R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "jtK92n7-4Xpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LVdPxs0o4jU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 7921768,
          "datasetId": 4538425,
          "sourceId": 7818440,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30664,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N0F7TJd_84ES"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}